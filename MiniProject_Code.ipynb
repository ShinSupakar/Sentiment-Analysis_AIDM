{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxKm2Yw2UC7F",
    "outputId": "39f64f67-0ed8-4467-d60a-35c15a7c9e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xyfdupEiVgrP",
    "outputId": "4d9d2243-ed9a-4bb4-9cb2-f12f4fe30438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn pandas imbalanced-learn nltk sentencepiece transformers torch gensim -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qrm2epWPTv82"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW   # AdamW Optimizer\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpKES3EvUI54",
    "outputId": "ba9b438e-072e-4e7e-ef35-c5859e8c0c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random seed set to 42 for reproducibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "#for reproducibility\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "print(\" Random seed set to 42 for reproducibility\")\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLMaIhXBUOwe",
    "outputId": "c73e0105-9512-4245-d060-0affefa18a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      " Dataset: 7401 samples\n",
      " Negative: 1082, Positive: 6319\n",
      "\n",
      "Applying text cleaning...\n",
      " Created 2 versions: traditional (aggressive) & minimal (for BERT)\n",
      "\n",
      "Splitting data...\n",
      " Training: 5920, Validation: 1481\n"
     ]
    }
   ],
   "source": [
    "#PART 1: Data Loading and Preprocessing\n",
    "\n",
    "# Load training data\n",
    "print(\"\\nLoading dataset...\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/AIDM /train.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\" Dataset: {len(df)} samples\")\n",
    "print(f\" Negative: {(df['sentiments']==0).sum()}, Positive: {(df['sentiments']==1).sum()}\")\n",
    "\n",
    "# Cleaning functions\n",
    "def clean_text_traditional(text):\n",
    "    \"\"\"Aggressive cleaning for TF-IDF/Word2Vec to removes stopwords, punctuation, numbers\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def clean_text_minimal(text):\n",
    "    \"\"\"Minimal cleaning for BERT to preserves punctuation and context\"\"\"\n",
    "    return str(text).strip()\n",
    "\n",
    "print(\"\\nApplying text cleaning...\")\n",
    "df['clean_reviews'] = df['reviews'].apply(clean_text_traditional)\n",
    "df['clean_minimal'] = df['reviews'].apply(clean_text_minimal)\n",
    "print(\" Created 2 versions: traditional (aggressive) & minimal (for BERT)\")\n",
    "\n",
    "# Train/validation split\n",
    "print(\"\\nSplitting data...\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['sentiments'])\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "print(f\" Training: {len(train_df)}, Validation: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfgsKLaFUTYH",
    "outputId": "88bdef52-8799-460d-d652-40bcfe68cc0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.1] TF-IDF + SMOTE\n",
      "Before SMOTE: (5920, 5000)\n",
      "After SMOTE: (10110, 5000)\n",
      "Balanced classes: Neg=5055, Pos=5055\n",
      "\n",
      "Training Logistic Regression on TF-IDF...\n",
      " TF-IDF + LR: Accuracy=0.8947, F1=0.9368\n"
     ]
    }
   ],
   "source": [
    "#Part2: Feature Engineering and Exploration\n",
    "\n",
    "\n",
    "# 2.1: TF-IDF + SMOTE (Baseline)\n",
    "\n",
    "print(\"\\n[2.1] TF-IDF + SMOTE\")\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['clean_reviews'])\n",
    "y_train = train_df['sentiments']\n",
    "\n",
    "print(f\"Before SMOTE: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Apply SMOTE for class balance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"After SMOTE: {X_train_smote.shape}\")\n",
    "print(f\"Balanced classes: Neg={(y_train_smote==0).sum()}, Pos={(y_train_smote==1).sum()}\")\n",
    "\n",
    "# Transform validation data\n",
    "X_val_tfidf = vectorizer.transform(val_df['clean_reviews'])\n",
    "y_val = val_df['sentiments']\n",
    "\n",
    "# Train baseline model\n",
    "print(\"\\nTraining Logistic Regression on TF-IDF...\")\n",
    "lr_tfidf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_tfidf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "val_pred_tfidf = lr_tfidf.predict(X_val_tfidf)\n",
    "val_acc_tfidf = accuracy_score(y_val, val_pred_tfidf)\n",
    "val_f1_tfidf = f1_score(y_val, val_pred_tfidf)\n",
    "\n",
    "print(f\" TF-IDF + LR: Accuracy={val_acc_tfidf:.4f}, F1={val_f1_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIKKb-_xUaOz",
    "outputId": "54688230-5c45-4052-de84-4b83121f9a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.2] Word2Vec Embeddings\n",
      "Tokenizing with NLTK...\n",
      " Tokenized 5920 samples\n",
      "Training Word2Vec (500 epochs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word2Vec complete - Semantic similarities:\n",
      "  good ↔ bad: 0.291\n",
      "  excellent ↔ terrible: 0.018\n",
      "  love ↔ hate: 0.070\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.2: Word2Vec Embeddings\n",
    "\n",
    "print(\"\\n[2.2] Word2Vec Embeddings\")\n",
    "\n",
    "\n",
    "print(\"Tokenizing with NLTK...\")\n",
    "tokenized_items = [word_tokenize(train_df.loc[idx, 'clean_reviews']) for idx in range(len(train_df))]\n",
    "print(f\" Tokenized {len(tokenized_items)} samples\")\n",
    "\n",
    "print(\"Training Word2Vec (500 epochs)...\")\n",
    "w2v = Word2Vec(tokenized_items, vector_size=100, window=5, min_count=1, workers=4)\n",
    "w2v.train(tokenized_items, total_examples=w2v.corpus_count, epochs=500)\n",
    "\n",
    "print(\" Word2Vec complete - Semantic similarities:\")\n",
    "print(f\"  good ↔ bad: {w2v.wv.similarity('good', 'bad'):.3f}\")\n",
    "try:\n",
    "    print(f\"  excellent ↔ terrible: {w2v.wv.similarity('excellent', 'terrible'):.3f}\")\n",
    "    print(f\"  love ↔ hate: {w2v.wv.similarity('love', 'hate'):.3f}\")\n",
    "except KeyError:\n",
    "    print(\"  (some words not in vocabulary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "referenced_widgets": [
      "f66bf3b022e14de9a8c97c44951f5321",
      "6c02b0a9ca5b4d5099ad4dc1be7a2f34",
      "eabf54e5c4d346268d73b13de11dd622",
      "482843161b534968bf59b65229809fe4",
      "611f7fa0812b44878f14f8b504c2027c",
      "7d1f178c161543c3893f75f1fc73bb55",
      "a973cee0a32f44ddb0b849bdafbb661d",
      "0f5d10bcad37416e96c766098195f195",
      "6c071d32665e4fd0b8c27a6b7713382f",
      "ca7fab694e0b489485c49dc7bbf491df",
      "b1ceef6244cb4714bd8dab8cf722afb0",
      "b4cc11d9c76e4fffb3a3d17ec2e1671a",
      "71581782000d4eb9b83aa88f21c94707",
      "f951ed50e2bd40fd819e8ade05264563",
      "c45ebbfc95d045d9ba476ac2ca25b4f3",
      "160e7db956e641d1a8d9141f1aa3a19c",
      "6c180fe59cd343cd918e42812d362773",
      "5e777e6f114f4eb892025cc9a8d339eb",
      "42884eeb52c34817a875c4387425503f",
      "b24f493329364674914932b1a253f5e9",
      "8f4c35dfde644bfe9fb216918fdeba93",
      "f2b0cea25d6c46599be892450480d9b9",
      "c6174fd241ec44769236bd012d8f5c20",
      "7f7352e2426a4248b0be3b93719e11ef",
      "aaf34292d47c41cbab0ef6e5eef5437e",
      "c38dc7b880e844a6b9581bd3e35e4781",
      "97223b7c2c5349719f86a7986e7e9bf9",
      "f421f87e3f5c4a939bd9e832366fea04",
      "7de356ea56024a15bca7ff784e13a4fd",
      "1f575d7f79994717beb9b2cc14d5f2fc",
      "6e8fc03543f74db99bbaa2ac18b893db",
      "9d5e91c2dba24d898decdb8bd29599fb",
      "2652dba2e487473aa971776339f9bfac"
     ]
    },
    "id": "JzgS2VMiUf3z",
    "outputId": "11572951-8e6d-4294-ac7b-b41ec430d6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.3] XLNet Tokenization (Exploration)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66bf3b022e14de9a8c97c44951f5321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cc11d9c76e4fffb3a3d17ec2e1671a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6174fd241ec44769236bd012d8f5c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample XLNet tokens: [2712, 9888, 3251, 14616, 115, 359, 1081, 20848, 18649, 9355, 1859, 9888, 9888, 15974, 23, 63, 88, 13468, 117, 3617]...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.3: XLNet Tokenization\n",
    "\n",
    "print(\"\\n[2.3] XLNet Tokenization (Exploration)\")\n",
    "\n",
    "xlnet_tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "sample_xlnet = xlnet_tokenizer(train_df.iloc[0]['clean_reviews'])\n",
    "print(f\" Sample XLNet tokens: {sample_xlnet['input_ids'][:20]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338,
     "referenced_widgets": [
      "0ada66a33b154605aafacb99d5b55999",
      "d558f2a58011447a92c4bfaf23003e39",
      "1ce8af7fbfb04201b85911e01fe9a501",
      "069c82da507a448daf7347395f1ecd29",
      "806d68a2871646588f1b84e941666107",
      "ab2f98d26038418394214136fe70a4e7",
      "509d96417c3140cdae1efb375c00b2c9",
      "48bca7a567fe4f5e87f93973c5fceb47",
      "93507162c5264e118bec0b016925922a",
      "6febc94685294611b0f8b0572fdb3f50",
      "269855690185483abb71eac0a249fb2b",
      "2a9b770dd33f4a1ca9b328a8f5dfc4d1",
      "8c616f424d144334b29132fbd2a8d833",
      "844962f5c607480c95b420e6a06f52d6",
      "56ef92a102e34c7d8b80604606fdccad",
      "81e13dc0eafe4c48a1a11835a6b39ef4",
      "7c812c6b9ce14fd8b9d34734f571c2b0",
      "b325d714b4ed4727a81a2232e357655c",
      "6d0e862485a84843bf03f435c7d3f373",
      "6cb89f3fcd384886acfb7456f83a301f",
      "8b6ec1ddcbe945a1a2c8ea1c090ce1c2",
      "f7974cf5ff00446e913897d7dd24fe66",
      "34c2cb43a3cb4893b867e090c7d8ee2a",
      "823bbc32ba964b2db7f3031f39333688",
      "5ee5216475e743d19bbe2394e649b4da",
      "d0a22cb91c5544d1aed4f0f8c2f27e8a",
      "858a99977de741a0930dfd7f7bd62f42",
      "638892c99dd349e89248dac26f52ebae",
      "025d2020ee5b428f89500883729457b6",
      "3722b674ef6541f6a3cb3a4406e6cef9",
      "05ae1ae3265c45e1a146f28d8cdb1e27",
      "9a8092c744444899b1d794a68fd5eb12",
      "dec1d410d00746568d390529d89f1168",
      "afc3eaa2be634bc8b85e13a20250016b",
      "85c4601df6e94666b27db08c0dcae32d",
      "33bc959d3ec34f118bc10e59ad2620a3",
      "d9b33447fe0942d197df6ddff7f472bb",
      "bfc69a8210fe402598c59a70a4c99a67",
      "9270f3a795204ed9a7f93e9435fbdd4c",
      "57074eeba7304ac88409125831f17675",
      "72f7ad171cc047328726e1e4642cad70",
      "e85b52f628e749fe888ca108c177e8d3",
      "b672efceb6a64b3585da8f20658c2323",
      "a87806af48ac49b1bc68fbb5a7c5f285",
      "2f8e3866ffec4946ad373d0e2152df68",
      "c43ad65d2e4349ac9182ce4846b2214d",
      "4f31c27aae8d4123a79db24aea3024d1",
      "67a0b22651ae492697da8de97fccf6d8",
      "2d52d279d4384eea924813977a345b63",
      "3d2abf9c521f400bb51947e036582748",
      "75372baf67de456c902f19f4095147bd",
      "81c365059daf4417b83089c446eb8942",
      "90da0840ff2e47c69cf55da9e0b83fae",
      "a474333bb35a4299955da8392ae2dbe8",
      "5ac4625ade4d44e3b4a9161c1bf59311"
     ]
    },
    "id": "09Mfs8KWUlfo",
    "outputId": "9acffa9f-9d14-4754-ac42-5211eb295bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.4] Frozen BERT Feature Extraction\n",
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ada66a33b154605aafacb99d5b55999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9b770dd33f4a1ca9b328a8f5dfc4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c2cb43a3cb4893b867e090c7d8ee2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc3eaa2be634bc8b85e13a20250016b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8e3866ffec4946ad373d0e2152df68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frozen BERT features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 185/185 [00:41<00:00,  4.41it/s]\n",
      "Extracting features: 100%|██████████| 47/47 [00:10<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Features shape: (5920, 768)\n",
      "Training Logistic Regression on frozen BERT features...\n",
      " Frozen BERT + LR: Accuracy=0.9196, F1=0.9533\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.4: Frozen BERT Features\n",
    "\n",
    "print(\"\\n[2.4] Frozen BERT Feature Extraction\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load BERT for feature extraction (frozen weights)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_frozen = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "bert_frozen.eval()\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(self.texts[idx]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def extract_bert_features(dataloader, model, device):\n",
    "    \"\"\"Extract [CLS] embeddings from frozen BERT\"\"\"\n",
    "    features, labels = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "            features.append(cls_embeddings)\n",
    "            labels.append(batch['labels'].numpy())\n",
    "\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "# Create datasets\n",
    "bert_train_dataset = BERTDataset(train_df['clean_minimal'].values, train_df['sentiments'].values, bert_tokenizer)\n",
    "bert_val_dataset = BERTDataset(val_df['clean_minimal'].values, val_df['sentiments'].values, bert_tokenizer)\n",
    "\n",
    "bert_train_loader = DataLoader(bert_train_dataset, batch_size=32, shuffle=False)\n",
    "bert_val_loader = DataLoader(bert_val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting frozen BERT features...\")\n",
    "X_train_bert_frozen, y_train_bert = extract_bert_features(bert_train_loader, bert_frozen, device)\n",
    "X_val_bert_frozen, y_val_bert = extract_bert_features(bert_val_loader, bert_frozen, device)\n",
    "print(f\" Features shape: {X_train_bert_frozen.shape}\")\n",
    "\n",
    "# Train classifier on frozen features\n",
    "print(\"Training Logistic Regression on frozen BERT features...\")\n",
    "lr_bert_frozen = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_bert_frozen.fit(X_train_bert_frozen, y_train_bert)\n",
    "\n",
    "val_pred_frozen = lr_bert_frozen.predict(X_val_bert_frozen)\n",
    "val_acc_frozen = accuracy_score(y_val_bert, val_pred_frozen)\n",
    "val_f1_frozen = f1_score(y_val_bert, val_pred_frozen)\n",
    "\n",
    "print(f\" Frozen BERT + LR: Accuracy={val_acc_frozen:.4f}, F1={val_f1_frozen:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4WBJWmbUuu-",
    "outputId": "8ba8971e-f09d-4478-f024-411655745a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration:\n",
      "  Batch Size: 16\n",
      "  Max Length: 128\n",
      "  Epochs: 4\n",
      "  Learning Rate: 2e-05\n",
      "  Optimizer: AdamW\n",
      "  Loss: CrossEntropyLoss (built-in)\n",
      "\n",
      "Loading BERT for fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 370/370 [02:20<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2275 | Train Acc: 0.9155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 93/93 [00:11<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1529 | Val Acc: 0.9548\n",
      " Best model saved!\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 370/370 [02:17<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0930 | Train Acc: 0.9709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 93/93 [00:11<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1715 | Val Acc: 0.9541\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 370/370 [02:17<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0367 | Train Acc: 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 93/93 [00:11<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2399 | Val Acc: 0.9534\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 370/370 [02:17<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0132 | Train Acc: 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 93/93 [00:11<00:00,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2652 | Val Acc: 0.9514\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 3: Fine Tuned Bert Neural Network\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 128\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Max Length: {MAX_LENGTH}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Loss: CrossEntropyLoss (built-in)\")\n",
    "\n",
    "# Load BERT for fine-tuning (all layers trainable)\n",
    "print(\"\\nLoading BERT for fine-tuning...\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Create dataloaders (with shuffling for training)\n",
    "train_dataset_finetune = BERTDataset(train_df['clean_minimal'].values, train_df['sentiments'].values, bert_tokenizer, MAX_LENGTH)\n",
    "val_dataset_finetune = BERTDataset(val_df['clean_minimal'].values, val_df['sentiments'].values, bert_tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader_finetune = DataLoader(train_dataset_finetune, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_finetune = DataLoader(val_dataset_finetune, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Optimizer and scheduler setup\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader_finetune) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training functions\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
    "    return accuracy, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            predictions_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(labels_list, predictions_list)\n",
    "    return accuracy, np.mean(losses), predictions_list, labels_list\n",
    "\n",
    "# Training\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "best_accuracy = 0\n",
    "history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model, train_loader_finetune, optimizer, scheduler, device)\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "\n",
    "    val_acc, val_loss, val_preds, val_labels = eval_model(model, val_loader_finetune, device)\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    history['train_acc'].append(train_acc.item() if torch.is_tensor(train_acc) else train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_finetuned_bert.pt')\n",
    "        best_accuracy = val_acc\n",
    "        print(f' Best model saved!')\n",
    "\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQ75a9naU12b",
    "outputId": "62e4c34b-6751-4bb8-baf8-afc89d6a4cda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 93/93 [00:11<00:00,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL COMPARISON\n",
      "--------------------------------------------------------------------------------\n",
      "Model                          Type            Accuracy     F1 Score    \n",
      "--------------------------------------------------------------------------------\n",
      "TF-IDF + SMOTE + LR            Baseline        0.8947       0.9368      \n",
      "Frozen BERT + LR               Intermediate    0.9196       0.9533      \n",
      "Fine-tuned BERT (NN)           Final Model     0.9548       0.9736      \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Best Model: Fine-tuned BERT\n",
      "  Accuracy: 0.9548\n",
      "  F1 Score: 0.9736\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CLASSIFICATION REPORT (Fine-tuned BERT)\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.83      0.84       217\n",
      "    Positive       0.97      0.98      0.97      1264\n",
      "\n",
      "    accuracy                           0.95      1481\n",
      "   macro avg       0.91      0.90      0.91      1481\n",
      "weighted avg       0.95      0.95      0.95      1481\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 180   37]\n",
      " [  30 1234]]\n",
      "\n",
      "[[TN=180, FP=37],\n",
      " [FN=30, TP=1234]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Part 4: Comparitive Analysis\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load('best_finetuned_bert.pt'))\n",
    "final_acc, final_loss, final_preds, final_labels = eval_model(model, val_loader_finetune, device)\n",
    "final_f1 = f1_score(final_labels, final_preds)\n",
    "\n",
    "# Comparison table\n",
    "results = {\n",
    "    'TF-IDF + SMOTE + LR': {'accuracy': val_acc_tfidf, 'f1_score': val_f1_tfidf, 'type': 'Baseline'},\n",
    "    'Frozen BERT + LR': {'accuracy': val_acc_frozen, 'f1_score': val_f1_frozen, 'type': 'Intermediate'},\n",
    "    'Fine-tuned BERT (NN)': {'accuracy': final_acc, 'f1_score': final_f1, 'type': 'Final Model'}\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Model':<30} {'Type':<15} {'Accuracy':<12} {'F1 Score':<12}\")\n",
    "print(\"-\"*80)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<30} {metrics['type']:<15} {metrics['accuracy']:<12.4f} {metrics['f1_score']:<12.4f}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\n Best Model: Fine-tuned BERT\")\n",
    "print(f\"  Accuracy: {final_acc:.4f}\")\n",
    "print(f\"  F1 Score: {final_f1:.4f}\")\n",
    "\n",
    "# Detailed evaluation\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CLASSIFICATION REPORT (Fine-tuned BERT)\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(final_labels, final_preds, target_names=['Negative', 'Positive']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(final_labels, final_preds)\n",
    "print(cm)\n",
    "print(f\"\\n[[TN={cm[0,0]}, FP={cm[0,1]}],\")\n",
    "print(f\" [FN={cm[1,0]}, TP={cm[1,1]}]]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSktX3yZU-SF",
    "outputId": "b8a1972b-471d-4518-efe1-3c4930483cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test data...\n",
      " Loaded 1851 test samples\n",
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 116/116 [00:17<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saved to 'submission.csv'\n",
      "  Total: 1851\n",
      "  Positive: 1595\n",
      "  Negative: 256\n",
      "\n",
      "Sample Predictions:\n",
      "--------------------------------------------------------------------------------\n",
      "1. Negative: I bought 2 sleepers.  sleeper had holes in the arm pit area and the other sleeper had a whole where ...\n",
      "2. Positive: I dare say these are just about the sexiest things I've ever worn. Oh I've had and have G-strings, h...\n",
      "3. Positive: everything about the transaction (price, delivery time, quality of item) was great.  I wouldn't hesi...\n",
      "4. Positive: Not bad for just a shirt.  Very durable, and matched my teams colors perfectly.  Its just a shirt, b...\n",
      "5. Positive: These are truly wrinkle free and longer than the average womans botton down, which I love!!   Overal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Test Set Predictions\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"\\nLoading test data...\")\n",
    "    with open('/content/drive/MyDrive/Colab Notebooks/AIDM /test.json', 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    print(f\" Loaded {len(test_df)} test samples\")\n",
    "\n",
    "    test_df['clean_minimal'] = test_df['reviews'].apply(clean_text_minimal)\n",
    "\n",
    "    test_dataset = BERTDataset(test_df['clean_minimal'].values, np.zeros(len(test_df)), bert_tokenizer, MAX_LENGTH)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            test_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # submission.csv\n",
    "    submission_df = pd.DataFrame({'review': test_df['reviews'].values, 'sentiment': test_predictions})\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "    print(f\"\\n Saved to 'submission.csv'\")\n",
    "    print(f\"  Total: {len(test_predictions)}\")\n",
    "    print(f\"  Positive: {(np.array(test_predictions)==1).sum()}\")\n",
    "    print(f\"  Negative: {(np.array(test_predictions)==0).sum()}\")\n",
    "\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i in range(min(5, len(submission_df))):\n",
    "        review = submission_df.iloc[i]['review'][:100] + \"...\"\n",
    "        sentiment = \"Positive\" if submission_df.iloc[i]['sentiment'] == 1 else \"Negative\"\n",
    "        print(f\"{i+1}. {sentiment}: {review}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n! test.json not found - skipping test predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qL0Ql9vmVDK8",
    "outputId": "88c2b7ad-97be-4424-8a7d-d908fea787f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Correctly Classified Examples ---\n",
      "\n",
      "Example 1 (Positive):\n",
      "Review: I love this bag!  The quality of the canvas and stitching is wonderful.  I would have expected to pay a lot more for this bag. The orange is a great color...\n",
      "\n",
      "Example 2 (Positive):\n",
      "Review: Great product at a great price!!  Fast shipping too!...\n",
      "\n",
      "Example 3 (Positive):\n",
      "Review: I've had two of these and they are great. Truly no headache! I'm here to buy a third...\n",
      "\n",
      "--- Incorrectly Classified Examples ---\n",
      "\n",
      "Example 1 (True: Negative, Predicted: Positive):\n",
      "Review: I found the band underneath the cups rolled under and wasn't very elastic...very uncomfortable.  It fit great when I tried it in the store\n",
      "(even had it fit to me by a Medela rep) but after a few weari...\n",
      "\n",
      "Example 2 (True: Negative, Predicted: Positive):\n",
      "Review: This is the only Haynes T-shirt that does not bunch at the back of the neck. It is light and drapes well. But it is too light for printing....\n",
      "\n",
      "Example 3 (True: Negative, Predicted: Positive):\n",
      "Review: This a very basic black boot. I choose these because I was 5 months pregnant at the time and wanted something a bit stylish yet low to the ground to help me avoid trips or falls. If you are into a ver...\n"
     ]
    }
   ],
   "source": [
    "# Part 6: Sample Analysis (Incorrect and Correct)\n",
    "\n",
    "\n",
    "val_df_analysis = val_df.copy().reset_index(drop=True)\n",
    "val_df_analysis['predictions'] = final_preds\n",
    "val_df_analysis['correct'] = val_df_analysis['sentiments'] == val_df_analysis['predictions']\n",
    "\n",
    "# Correctly classified\n",
    "correct_samples = val_df_analysis[val_df_analysis['correct']]\n",
    "print(\"\\n--- Correctly Classified Examples ---\")\n",
    "for i in range(min(3, len(correct_samples))):\n",
    "    sample = correct_samples.iloc[i]\n",
    "    label = \"Positive\" if sample['sentiments'] == 1 else \"Negative\"\n",
    "    print(f\"\\nExample {i+1} ({label}):\")\n",
    "    print(f\"Review: {sample['reviews'][:200]}...\")\n",
    "\n",
    "# Incorrectly classified\n",
    "incorrect_samples = val_df_analysis[~val_df_analysis['correct']]\n",
    "if len(incorrect_samples) > 0:\n",
    "    print(\"\\n--- Incorrectly Classified Examples ---\")\n",
    "    for i in range(min(3, len(incorrect_samples))):\n",
    "        sample = incorrect_samples.iloc[i]\n",
    "        true_label = \"Positive\" if sample['sentiments'] == 1 else \"Negative\"\n",
    "        pred_label = \"Positive\" if sample['predictions'] == 1 else \"Negative\"\n",
    "        print(f\"\\nExample {i+1} (True: {true_label}, Predicted: {pred_label}):\")\n",
    "        print(f\"Review: {sample['reviews'][:200]}...\")\n",
    "else:\n",
    "    print(\"\\n Perfect classification - no errors!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
